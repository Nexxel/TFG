=======================================
Simulation: 1
Iteration: 1
----------
State: 1
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 1
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 1
New value of Q matrix: 0.2
New value of Value function: 0.2
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 2
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 1
New value of Q matrix: 0.2
New value of Value function: 0.2
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 3
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 1
New value of Q matrix: 0.2036
New value of Value function: 0.2036
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 4
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 1
New value of Q matrix: 0.203665
New value of Value function: 0.203665
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 5
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 2
New value of Q matrix: 0.403257
New value of Value function: 0.403257
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 6
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 3
New value of Q matrix: 0.602451
New value of Value function: 0.602451
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 7
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 4
New value of Q matrix: 0.801246
New value of Value function: 0.801246
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 8
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 5
New value of Q matrix: 0.999644
New value of Value function: 0.999644
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 9
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 6
New value of Q matrix: 1.19764
New value of Value function: 1.19764
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 10
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 7
New value of Q matrix: 1.39525
New value of Value function: 1.39525
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 11
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 8
New value of Q matrix: 1.59246
New value of Value function: 1.59246
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 12
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 2
New value of Q matrix: 0.428192
New value of Value function: 1.59246
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 13
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 9
New value of Q matrix: 1.78927
New value of Value function: 1.78927
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 14
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 10
New value of Q matrix: 1.9857
New value of Value function: 1.9857
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 15
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 3
New value of Q matrix: 0.655371
New value of Value function: 1.9857
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 16
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 11
New value of Q matrix: 2.18172
New value of Value function: 2.18172
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 17
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 12
New value of Q matrix: 2.37736
New value of Value function: 2.37736
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 18
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 13
New value of Q matrix: 2.57261
New value of Value function: 2.57261
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 19
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 14
New value of Q matrix: 2.76746
New value of Value function: 2.76746
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 20
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 15
New value of Q matrix: 2.96193
New value of Value function: 2.96193
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 21
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 16
New value of Q matrix: 3.156
New value of Value function: 3.156
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 22
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 17
New value of Q matrix: 3.34969
New value of Value function: 3.34969
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 23
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 18
New value of Q matrix: 3.54299
New value of Value function: 3.54299
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 24
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 19
New value of Q matrix: 3.7359
New value of Value function: 3.7359
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 25
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 20
New value of Q matrix: 3.92843
New value of Value function: 3.92843
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 26
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 2
New value of Q matrix: 0.466712
New value of Value function: 3.92843
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 27
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 21
New value of Q matrix: 4.12058
New value of Value function: 4.12058
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 28
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 22
New value of Q matrix: 4.31233
New value of Value function: 4.31233
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 29
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 23
New value of Q matrix: 4.50371
New value of Value function: 4.50371
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 30
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 24
New value of Q matrix: 4.6947
New value of Value function: 4.6947
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 31
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 25
New value of Q matrix: 4.88531
New value of Value function: 4.88531
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 32
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 26
New value of Q matrix: 5.07554
New value of Value function: 5.07554
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 33
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 1
New value of Q matrix: 0.29136
New value of Value function: 5.07554
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 34
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 35
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 27
New value of Q matrix: 5.26539
New value of Value function: 5.26539
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 36
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 2
New value of Q matrix: 0.58031
New value of Value function: 5.26539
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 37
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 38
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 39
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 40
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 41
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 42
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 5
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 43
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 44
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 45
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 46
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 47
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 28
New value of Q matrix: 5.45486
New value of Value function: 5.45486
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 48
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 29
New value of Q matrix: 5.64395
New value of Value function: 5.64395
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 49
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 30
New value of Q matrix: 5.83266
New value of Value function: 5.83266
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 50
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 31
New value of Q matrix: 6.021
New value of Value function: 6.021
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 51
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 32
New value of Q matrix: 6.20896
New value of Value function: 6.20896
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 52
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 33
New value of Q matrix: 6.39654
New value of Value function: 6.39654
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 53
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 3
New value of Q matrix: 0.772515
New value of Value function: 6.39654
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 54
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 4
New value of Q matrix: 1.0722
New value of Value function: 6.39654
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 55
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 4
New value of Q matrix: 0.957401
New value of Value function: 6.39654
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 56
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 3
New value of Q matrix: 0.883841
New value of Value function: 6.39654
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 57
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 5
New value of Q matrix: 1.25339
New value of Value function: 6.39654
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 58
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 34
New value of Q matrix: 6.58374
New value of Value function: 6.58374
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 59
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 35
New value of Q matrix: 6.77058
New value of Value function: 6.77058
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 60
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 36
New value of Q matrix: 6.95704
New value of Value function: 6.95704
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 61
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 37
New value of Q matrix: 7.14312
New value of Value function: 7.14312
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 62
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 38
New value of Q matrix: 7.32884
New value of Value function: 7.32884
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 63
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 39
New value of Q matrix: 7.51418
New value of Value function: 7.51418
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 64
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 40
New value of Q matrix: 7.69915
New value of Value function: 7.69915
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 65
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 41
New value of Q matrix: 7.88375
New value of Value function: 7.88375
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 66
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 42
New value of Q matrix: 8.06798
New value of Value function: 8.06798
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 67
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 43
New value of Q matrix: 8.25185
New value of Value function: 8.25185
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 68
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 44
New value of Q matrix: 8.43534
New value of Value function: 8.43534
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 69
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 45
New value of Q matrix: 8.61847
New value of Value function: 8.61847
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 70
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 46
New value of Q matrix: 8.80124
New value of Value function: 8.80124
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 71
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 47
New value of Q matrix: 8.98363
New value of Value function: 8.98363
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 72
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 48
New value of Q matrix: 9.16567
New value of Value function: 9.16567
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 73
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 49
New value of Q matrix: 9.34734
New value of Value function: 9.34734
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 74
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 50
New value of Q matrix: 9.52864
New value of Value function: 9.52864
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 75
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 51
New value of Q matrix: 9.70958
New value of Value function: 9.70958
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 76
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 52
New value of Q matrix: 9.89016
New value of Value function: 9.89016
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 77
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 53
New value of Q matrix: 10.0704
New value of Value function: 10.0704
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 78
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 54
New value of Q matrix: 10.2502
New value of Value function: 10.2502
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 79
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 55
New value of Q matrix: 10.4297
New value of Value function: 10.4297
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 80
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 56
New value of Q matrix: 10.6089
New value of Value function: 10.6089
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 81
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 6
New value of Q matrix: 1.61928
New value of Value function: 10.6089
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 82
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 7
New value of Q matrix: 1.97786
New value of Value function: 10.6089
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 83
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 57
New value of Q matrix: 10.7877
New value of Value function: 10.7877
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 84
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 58
New value of Q matrix: 10.9661
New value of Value function: 10.9661
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 85
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 59
New value of Q matrix: 11.1442
New value of Value function: 11.1442
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 86
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 8
New value of Q matrix: 2.33889
New value of Value function: 11.1442
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 87
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 60
New value of Q matrix: 11.3219
New value of Value function: 11.3219
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 88
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 61
New value of Q matrix: 11.4992
New value of Value function: 11.4992
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 89
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 4
New value of Q matrix: 1.27315
New value of Value function: 11.4992
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 90
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 62
New value of Q matrix: 11.6762
New value of Value function: 11.6762
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 91
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 9
New value of Q matrix: 2.70229
New value of Value function: 11.6762
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 92
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 63
New value of Q matrix: 11.8529
New value of Value function: 11.8529
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 93
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 64
New value of Q matrix: 12.0292
New value of Value function: 12.0292
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 94
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 65
New value of Q matrix: 12.2051
New value of Value function: 12.2051
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 95
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 66
New value of Q matrix: 12.3807
New value of Value function: 12.3807
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 96
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 67
New value of Q matrix: 12.5559
New value of Value function: 12.5559
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 97
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 68
New value of Q matrix: 12.7308
New value of Value function: 12.7308
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 98
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 5
New value of Q matrix: 1.67684
New value of Value function: 12.7308
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 99
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 100
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 101
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 102
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 103
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 104
----------
State: 160
	Distance: 0
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 160
	Distance: 0
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 105
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 106
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 69
New value of Q matrix: 12.9054
New value of Value function: 12.9054
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 107
----------
State: 160
	Distance: 0
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 160
	Distance: 0
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 108
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 10
New value of Q matrix: 3.08054
New value of Value function: 12.9054
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 109
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 70
New value of Q matrix: 13.0796
New value of Value function: 13.0796
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 110
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 71
New value of Q matrix: 13.2534
New value of Value function: 13.2534
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 111
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 72
New value of Q matrix: 13.4269
New value of Value function: 13.4269
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 112
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 73
New value of Q matrix: 13.6
New value of Value function: 13.6
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 113
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 5
New value of Q matrix: 1.49556
New value of Value function: 13.6
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 114
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 11
New value of Q matrix: 3.46373
New value of Value function: 13.6
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 115
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 74
New value of Q matrix: 13.7728
New value of Value function: 13.7728
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 116
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 75
New value of Q matrix: 13.9453
New value of Value function: 13.9453
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 117
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 76
New value of Q matrix: 14.1174
New value of Value function: 14.1174
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 118
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 77
New value of Q matrix: 14.2892
New value of Value function: 14.2892
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 119
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 78
New value of Q matrix: 14.4606
New value of Value function: 14.4606
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 120
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 12
New value of Q matrix: 3.85475
New value of Value function: 14.4606
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 121
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 79
New value of Q matrix: 14.6317
New value of Value function: 14.6317
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 122
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 6
New value of Q matrix: 2.10668
New value of Value function: 14.6317
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 123
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 7
New value of Q matrix: 2.52791
New value of Value function: 14.6317
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 124
----------
State: 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 125
----------
State: 160
	Distance: 0
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 160
	Distance: 0
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 126
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 127
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 128
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 80
New value of Q matrix: 14.8024
New value of Value function: 14.8024
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 129
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 130
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 13
New value of Q matrix: 4.24409
New value of Value function: 14.8024
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 131
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 14
New value of Q matrix: 4.62566
New value of Value function: 14.8024
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 132
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 81
New value of Q matrix: 14.9728
New value of Value function: 14.9728
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 133
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 82
New value of Q matrix: 15.1429
New value of Value function: 15.1429
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 134
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 83
New value of Q matrix: 15.3126
New value of Value function: 15.3126
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 135
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 84
New value of Q matrix: 15.4819
New value of Value function: 15.4819
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 136
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 8
New value of Q matrix: 2.95603
New value of Value function: 15.4819
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 137
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 85
New value of Q matrix: 15.651
New value of Value function: 15.651
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 138
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 86
New value of Q matrix: 15.8197
New value of Value function: 15.8197
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 139
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 87
New value of Q matrix: 15.988
New value of Value function: 15.988
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 140
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 88
New value of Q matrix: 16.1561
New value of Value function: 16.1561
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 141
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 89
New value of Q matrix: 16.3237
New value of Value function: 16.3237
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 142
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 90
New value of Q matrix: 16.4911
New value of Value function: 16.4911
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 143
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 91
New value of Q matrix: 16.6581
New value of Value function: 16.6581
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 144
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 92
New value of Q matrix: 16.8248
New value of Value function: 16.8248
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 145
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 15
New value of Q matrix: 5.03599
New value of Value function: 16.8248
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 146
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 93
New value of Q matrix: 16.9912
New value of Value function: 16.9912
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 147
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 94
New value of Q matrix: 17.1572
New value of Value function: 17.1572
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 148
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 95
New value of Q matrix: 17.3229
New value of Value function: 17.3229
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 149
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 96
New value of Q matrix: 17.4882
New value of Value function: 17.4882
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 150
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 97
New value of Q matrix: 17.6532
New value of Value function: 17.6532
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 151
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 98
New value of Q matrix: 17.8179
New value of Value function: 17.8179
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 152
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 99
New value of Q matrix: 17.9823
New value of Value function: 17.9823
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 153
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 100
New value of Q matrix: 18.1463
New value of Value function: 18.1463
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 154
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 101
New value of Q matrix: 18.31
New value of Value function: 18.31
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 155
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 16
New value of Q matrix: 5.46485
New value of Value function: 18.31
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 156
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 102
New value of Q matrix: 18.4734
New value of Value function: 18.4734
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 157
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 9
New value of Q matrix: 3.42943
New value of Value function: 18.4734
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 158
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 103
New value of Q matrix: 18.6365
New value of Value function: 18.6365
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 159
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 10
New value of Q matrix: 3.8963
New value of Value function: 18.6365
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 160
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 161
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 162
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 163
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 104
New value of Q matrix: 18.7992
New value of Value function: 18.7992
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 164
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 105
New value of Q matrix: 18.9616
New value of Value function: 18.9616
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 165
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 106
New value of Q matrix: 19.1237
New value of Value function: 19.1237
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 166
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 107
New value of Q matrix: 19.2854
New value of Value function: 19.2854
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 167
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 108
New value of Q matrix: 19.4469
New value of Value function: 19.4469
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 168
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 109
New value of Q matrix: 19.608
New value of Value function: 19.608
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 169
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 110
New value of Q matrix: 19.7687
New value of Value function: 19.7687
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 170
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 111
New value of Q matrix: 19.9292
New value of Value function: 19.9292
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 171
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 6
New value of Q matrix: 2.02437
New value of Value function: 19.9292
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 172
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 112
New value of Q matrix: 20.0893
New value of Value function: 20.0893
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 173
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 17
New value of Q matrix: 5.91716
New value of Value function: 20.0893
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 174
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 113
New value of Q matrix: 20.2492
New value of Value function: 20.2492
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 175
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 11
New value of Q matrix: 4.38286
New value of Value function: 20.2492
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 176
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 12
New value of Q matrix: 4.85968
New value of Value function: 20.2492
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 177
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 178
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 179
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 180
----------
State: 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 181
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 182
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 114
New value of Q matrix: 20.4087
New value of Value function: 20.4087
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 183
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 184
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 13
New value of Q matrix: 5.32985
New value of Value function: 20.4087
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 185
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 186
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 5
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 187
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 7
New value of Q matrix: 2.55124
New value of Value function: 20.4087
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 188
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 115
New value of Q matrix: 20.5679
New value of Value function: 20.5679
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 189
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 116
New value of Q matrix: 20.7267
New value of Value function: 20.7267
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 190
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 117
New value of Q matrix: 20.8853
New value of Value function: 20.8853
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 191
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 118
New value of Q matrix: 21.0435
New value of Value function: 21.0435
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 192
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 119
New value of Q matrix: 21.2014
New value of Value function: 21.2014
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 193
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 120
New value of Q matrix: 21.359
New value of Value function: 21.359
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 194
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 121
New value of Q matrix: 21.5163
New value of Value function: 21.5163
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 195
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 122
New value of Q matrix: 21.6733
New value of Value function: 21.6733
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 196
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 123
New value of Q matrix: 21.8299
New value of Value function: 21.8299
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 197
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 124
New value of Q matrix: 21.9862
New value of Value function: 21.9862
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 198
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 125
New value of Q matrix: 22.1423
New value of Value function: 22.1423
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 199
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 126
New value of Q matrix: 22.298
New value of Value function: 22.298
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 200
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 127
New value of Q matrix: 22.4534
New value of Value function: 22.4534
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 201
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 128
New value of Q matrix: 22.6085
New value of Value function: 22.6085
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 202
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 129
New value of Q matrix: 22.7633
New value of Value function: 22.7633
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 203
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 130
New value of Q matrix: 22.9177
New value of Value function: 22.9177
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 204
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 131
New value of Q matrix: 23.0719
New value of Value function: 23.0719
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 205
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 132
New value of Q matrix: 23.2258
New value of Value function: 23.2258
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 206
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 133
New value of Q matrix: 23.3793
New value of Value function: 23.3793
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 207
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 134
New value of Q matrix: 23.5326
New value of Value function: 23.5326
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 208
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 135
New value of Q matrix: 23.6855
New value of Value function: 23.6855
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 209
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 136
New value of Q matrix: 23.8381
New value of Value function: 23.8381
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 210
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 137
New value of Q matrix: 23.9904
New value of Value function: 23.9904
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 211
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 138
New value of Q matrix: 24.1425
New value of Value function: 24.1425
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 212
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 139
New value of Q matrix: 24.2942
New value of Value function: 24.2942
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 213
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 14
New value of Q matrix: 5.86054
New value of Value function: 24.2942
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 214
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 215
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 216
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 140
New value of Q matrix: 24.4456
New value of Value function: 24.4456
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 217
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 141
New value of Q matrix: 24.5967
New value of Value function: 24.5967
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 218
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 142
New value of Q matrix: 24.7475
New value of Value function: 24.7475
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 219
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 143
New value of Q matrix: 24.898
New value of Value function: 24.898
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 220
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 144
New value of Q matrix: 25.0482
New value of Value function: 25.0482
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 221
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 145
New value of Q matrix: 25.1981
New value of Value function: 25.1981
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 222
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 146
New value of Q matrix: 25.3477
New value of Value function: 25.3477
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 223
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 147
New value of Q matrix: 25.497
New value of Value function: 25.497
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 224
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 148
New value of Q matrix: 25.646
New value of Value function: 25.646
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 225
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 149
New value of Q matrix: 25.7947
New value of Value function: 25.7947
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 226
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 8
New value of Q matrix: 3.16452
New value of Value function: 25.7947
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 227
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 150
New value of Q matrix: 25.9431
New value of Value function: 25.9431
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 228
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 151
New value of Q matrix: 26.0913
New value of Value function: 26.0913
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 229
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 152
New value of Q matrix: 26.2391
New value of Value function: 26.2391
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 230
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 153
New value of Q matrix: 26.3866
New value of Value function: 26.3866
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 231
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 154
New value of Q matrix: 26.5338
New value of Value function: 26.5338
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 232
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 155
New value of Q matrix: 26.6808
New value of Value function: 26.6808
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 233
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 9
New value of Q matrix: 3.78149
New value of Value function: 26.6808
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 234
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 156
New value of Q matrix: 26.8274
New value of Value function: 26.8274
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 235
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 157
New value of Q matrix: 26.9737
New value of Value function: 26.9737
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 236
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 158
New value of Q matrix: 27.1198
New value of Value function: 27.1198
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 237
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 159
New value of Q matrix: 27.2656
New value of Value function: 27.2656
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 238
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 160
New value of Q matrix: 27.411
New value of Value function: 27.411
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 239
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 161
New value of Q matrix: 27.5562
New value of Value function: 27.5562
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 240
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 15
New value of Q matrix: 6.43935
New value of Value function: 27.5562
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 241
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 242
----------
State: 736
	Distance: 4
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 736
	Distance: 4
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 243
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 162
New value of Q matrix: 27.7011
New value of Value function: 27.7011
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 244
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 163
New value of Q matrix: 27.8457
New value of Value function: 27.8457
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 245
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 18
New value of Q matrix: 6.50004
New value of Value function: 27.8457
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 246
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 164
New value of Q matrix: 27.99
New value of Value function: 27.99
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 247
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 165
New value of Q matrix: 28.134
New value of Value function: 28.134
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 248
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 166
New value of Q matrix: 28.2777
New value of Value function: 28.2777
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 249
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 167
New value of Q matrix: 28.4212
New value of Value function: 28.4212
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 250
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 168
New value of Q matrix: 28.5644
New value of Value function: 28.5644
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 251
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 16
New value of Q matrix: 7.02472
New value of Value function: 28.5644
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 252
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 169
New value of Q matrix: 28.7072
New value of Value function: 28.7072
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 253
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 19
New value of Q matrix: 7.08677
New value of Value function: 28.7072
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 254
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 170
New value of Q matrix: 28.8498
New value of Value function: 28.8498
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 255
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 171
New value of Q matrix: 28.9921
New value of Value function: 28.9921
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 256
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 172
New value of Q matrix: 29.1341
New value of Value function: 29.1341
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 257
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 10
New value of Q matrix: 4.43027
New value of Value function: 29.1341
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 258
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 173
New value of Q matrix: 29.2759
New value of Value function: 29.2759
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 259
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 174
New value of Q matrix: 29.4173
New value of Value function: 29.4173
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 260
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 175
New value of Q matrix: 29.5585
New value of Value function: 29.5585
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 261
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 11
New value of Q matrix: 5.07372
New value of Value function: 29.5585
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 262
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 176
New value of Q matrix: 29.6994
New value of Value function: 29.6994
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 263
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 177
New value of Q matrix: 29.84
New value of Value function: 29.84
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 264
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 178
New value of Q matrix: 29.9803
New value of Value function: 29.9803
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 265
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 179
New value of Q matrix: 30.1203
New value of Value function: 30.1203
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 266
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 20
New value of Q matrix: 7.6872
New value of Value function: 30.1203
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 267
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 180
New value of Q matrix: 30.2601
New value of Value function: 30.2601
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 268
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 181
New value of Q matrix: 30.3996
New value of Value function: 30.3996
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 269
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 182
New value of Q matrix: 30.5388
New value of Value function: 30.5388
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 270
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 12
New value of Q matrix: 5.72194
New value of Value function: 30.5388
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 271
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 183
New value of Q matrix: 30.6777
New value of Value function: 30.6777
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 272
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 21
New value of Q matrix: 8.28565
New value of Value function: 30.6777
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 273
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 184
New value of Q matrix: 30.8163
New value of Value function: 30.8163
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 274
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 185
New value of Q matrix: 30.9547
New value of Value function: 30.9547
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 275
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 186
New value of Q matrix: 31.0928
New value of Value function: 31.0928
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 276
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 17
New value of Q matrix: 7.64389
New value of Value function: 31.0928
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 277
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 187
New value of Q matrix: 31.2306
New value of Value function: 31.2306
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 278
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 188
New value of Q matrix: 31.3681
New value of Value function: 31.3681
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 279
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 189
New value of Q matrix: 31.5054
New value of Value function: 31.5054
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 280
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 190
New value of Q matrix: 31.6424
New value of Value function: 31.6424
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 281
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 191
New value of Q matrix: 31.7791
New value of Value function: 31.7791
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 282
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 192
New value of Q matrix: 31.9155
New value of Value function: 31.9155
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 283
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 193
New value of Q matrix: 32.0517
New value of Value function: 32.0517
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 284
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 22
New value of Q matrix: 8.89687
New value of Value function: 32.0517
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 285
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 194
New value of Q matrix: 32.1876
New value of Value function: 32.1876
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 286
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 18
New value of Q matrix: 8.27039
New value of Value function: 32.1876
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 287
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 195
New value of Q matrix: 32.3232
New value of Value function: 32.3232
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 288
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 196
New value of Q matrix: 32.4586
New value of Value function: 32.4586
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 289
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 197
New value of Q matrix: 32.5937
New value of Value function: 32.5937
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 290
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 198
New value of Q matrix: 32.7285
New value of Value function: 32.7285
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 291
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 23
New value of Q matrix: 9.50805
New value of Value function: 32.7285
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 292
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 199
New value of Q matrix: 32.863
New value of Value function: 32.863
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 293
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 200
New value of Q matrix: 32.9973
New value of Value function: 32.9973
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 294
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 201
New value of Q matrix: 33.1313
New value of Value function: 33.1313
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 295
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 202
New value of Q matrix: 33.265
New value of Value function: 33.265
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 296
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 19
New value of Q matrix: 8.90375
New value of Value function: 33.265
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 297
----------
State: 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 298
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 6
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 299
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 24
New value of Q matrix: 10.1167
New value of Value function: 33.265
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 300
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 203
New value of Q matrix: 33.3985
New value of Value function: 33.3985
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 301
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 204
New value of Q matrix: 33.5317
New value of Value function: 33.5317
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 302
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 205
New value of Q matrix: 33.6647
New value of Value function: 33.6647
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 303
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 206
New value of Q matrix: 33.7973
New value of Value function: 33.7973
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 304
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 207
New value of Q matrix: 33.9297
New value of Value function: 33.9297
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 305
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 208
New value of Q matrix: 34.0619
New value of Value function: 34.0619
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 306
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 209
New value of Q matrix: 34.1937
New value of Value function: 34.1937
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 307
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 210
New value of Q matrix: 34.3254
New value of Value function: 34.3254
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 308
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 211
New value of Q matrix: 34.4567
New value of Value function: 34.4567
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 309
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 212
New value of Q matrix: 34.5878
New value of Value function: 34.5878
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 310
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 213
New value of Q matrix: 34.7186
New value of Value function: 34.7186
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 311
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 13
New value of Q matrix: 6.43244
New value of Value function: 34.7186
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 312
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 214
New value of Q matrix: 34.8492
New value of Value function: 34.8492
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 313
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 25
New value of Q matrix: 10.7416
New value of Value function: 34.8492
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 314
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 14
New value of Q matrix: 7.13107
New value of Value function: 34.8492
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 315
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 215
New value of Q matrix: 34.9795
New value of Value function: 34.9795
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 316
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 15
New value of Q matrix: 7.81808
New value of Value function: 34.9795
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 317
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 216
New value of Q matrix: 35.1095
New value of Value function: 35.1095
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 318
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 217
New value of Q matrix: 35.2393
New value of Value function: 35.2393
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 319
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 26
New value of Q matrix: 11.3611
New value of Value function: 35.2393
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 320
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 218
New value of Q matrix: 35.3688
New value of Value function: 35.3688
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 321
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 219
New value of Q matrix: 35.4981
New value of Value function: 35.4981
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 322
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 220
New value of Q matrix: 35.6271
New value of Value function: 35.6271
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 323
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 221
New value of Q matrix: 35.7558
New value of Value function: 35.7558
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 324
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 222
New value of Q matrix: 35.8843
New value of Value function: 35.8843
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 325
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 223
New value of Q matrix: 36.0126
New value of Value function: 36.0126
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 326
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 16
New value of Q matrix: 8.50995
New value of Value function: 36.0126
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 327
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 27
New value of Q matrix: 11.9821
New value of Value function: 36.0126
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 328
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 224
New value of Q matrix: 36.1405
New value of Value function: 36.1405
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 329
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 225
New value of Q matrix: 36.2683
New value of Value function: 36.2683
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 330
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 226
New value of Q matrix: 36.3957
New value of Value function: 36.3957
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 331
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 227
New value of Q matrix: 36.5229
New value of Value function: 36.5229
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 332
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 228
New value of Q matrix: 36.6499
New value of Value function: 36.6499
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 333
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 229
New value of Q matrix: 36.7766
New value of Value function: 36.7766
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 334
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 230
New value of Q matrix: 36.903
New value of Value function: 36.903
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 335
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 231
New value of Q matrix: 37.0292
New value of Value function: 37.0292
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 336
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 232
New value of Q matrix: 37.1552
New value of Value function: 37.1552
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 337
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 233
New value of Q matrix: 37.2808
New value of Value function: 37.2808
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 338
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 17
New value of Q matrix: 9.2108
New value of Value function: 37.2808
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 339
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 234
New value of Q matrix: 37.4063
New value of Value function: 37.4063
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 340
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 235
New value of Q matrix: 37.5315
New value of Value function: 37.5315
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 341
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 236
New value of Q matrix: 37.6564
New value of Value function: 37.6564
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 342
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 237
New value of Q matrix: 37.7811
New value of Value function: 37.7811
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 343
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 238
New value of Q matrix: 37.9055
New value of Value function: 37.9055
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 344
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 239
New value of Q matrix: 38.0297
New value of Value function: 38.0297
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 345
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 240
New value of Q matrix: 38.1537
New value of Value function: 38.1537
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 346
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 20
New value of Q matrix: 9.61245
New value of Value function: 38.1537
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 347
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 5
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 348
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 349
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 350
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 6
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 351
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 7
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 352
----------
State: 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 353
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 7
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 354
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 241
New value of Q matrix: 38.2774
New value of Value function: 38.2774
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 355
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 8
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 356
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 242
New value of Q matrix: 38.4008
New value of Value function: 38.4008
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 357
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 9
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 358
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 243
New value of Q matrix: 38.524
New value of Value function: 38.524
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 359
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 360
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 10
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 361
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 362
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 244
New value of Q matrix: 38.647
New value of Value function: 38.647
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 363
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 245
New value of Q matrix: 38.7697
New value of Value function: 38.7697
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 364
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 246
New value of Q matrix: 38.8921
New value of Value function: 38.8921
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 365
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 247
New value of Q matrix: 39.0143
New value of Value function: 39.0143
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 366
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 248
New value of Q matrix: 39.1363
New value of Value function: 39.1363
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 367
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 249
New value of Q matrix: 39.258
New value of Value function: 39.258
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 368
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 250
New value of Q matrix: 39.3795
New value of Value function: 39.3795
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 369
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 251
New value of Q matrix: 39.5008
New value of Value function: 39.5008
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 370
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 252
New value of Q matrix: 39.6218
New value of Value function: 39.6218
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 371
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 253
New value of Q matrix: 39.7425
New value of Value function: 39.7425
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 372
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 254
New value of Q matrix: 39.863
New value of Value function: 39.863
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 373
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 255
New value of Q matrix: 39.9833
New value of Value function: 39.9833
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 374
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 28
New value of Q matrix: 12.6621
New value of Value function: 39.9833
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 375
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 256
New value of Q matrix: 40.1033
New value of Value function: 40.1033
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 376
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 257
New value of Q matrix: 40.2231
New value of Value function: 40.2231
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 377
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 21
New value of Q matrix: 10.3442
New value of Value function: 40.2231
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 378
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 258
New value of Q matrix: 40.3427
New value of Value function: 40.3427
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 379
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 11
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 380
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 12
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 381
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 259
New value of Q matrix: 40.462
New value of Value function: 40.462
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 382
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 260
New value of Q matrix: 40.5811
New value of Value function: 40.5811
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 383
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 13
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 384
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 261
New value of Q matrix: 40.6999
New value of Value function: 40.6999
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 385
----------
State: 160
	Distance: 0
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 160
	Distance: 0
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 386
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 262
New value of Q matrix: 40.8185
New value of Value function: 40.8185
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 387
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 263
New value of Q matrix: 40.9369
New value of Value function: 40.9369
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 388
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 264
New value of Q matrix: 41.055
New value of Value function: 41.055
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 389
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 265
New value of Q matrix: 41.1729
New value of Value function: 41.1729
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 390
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 29
New value of Q matrix: 13.35
New value of Value function: 41.1729
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 391
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 266
New value of Q matrix: 41.2905
New value of Value function: 41.2905
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 392
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 267
New value of Q matrix: 41.408
New value of Value function: 41.408
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 393
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 268
New value of Q matrix: 41.5251
New value of Value function: 41.5251
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 394
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 269
New value of Q matrix: 41.6421
New value of Value function: 41.6421
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 395
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 30
New value of Q matrix: 14.0326
New value of Value function: 41.6421
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 396
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 270
New value of Q matrix: 41.7588
New value of Value function: 41.7588
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 397
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 271
New value of Q matrix: 41.8753
New value of Value function: 41.8753
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 398
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 18
New value of Q matrix: 9.98034
New value of Value function: 41.8753
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 399
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 272
New value of Q matrix: 41.9915
New value of Value function: 41.9915
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 400
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 273
New value of Q matrix: 42.1076
New value of Value function: 42.1076
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 401
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 274
New value of Q matrix: 42.2233
New value of Value function: 42.2233
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 402
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 275
New value of Q matrix: 42.3389
New value of Value function: 42.3389
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 403
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 276
New value of Q matrix: 42.4542
New value of Value function: 42.4542
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 404
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 277
New value of Q matrix: 42.5693
New value of Value function: 42.5693
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 405
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 278
New value of Q matrix: 42.6842
New value of Value function: 42.6842
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 406
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 279
New value of Q matrix: 42.7988
New value of Value function: 42.7988
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 407
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 280
New value of Q matrix: 42.9132
New value of Value function: 42.9132
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 408
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 19
New value of Q matrix: 10.7532
New value of Value function: 42.9132
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 409
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 281
New value of Q matrix: 43.0274
New value of Value function: 43.0274
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 410
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 20
New value of Q matrix: 11.5126
New value of Value function: 43.0274
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 411
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 31
New value of Q matrix: 14.7264
New value of Value function: 43.0274
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 412
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 282
New value of Q matrix: 43.1413
New value of Value function: 43.1413
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 413
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 283
New value of Q matrix: 43.255
New value of Value function: 43.255
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 414
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 284
New value of Q matrix: 43.3685
New value of Value function: 43.3685
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 415
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 10
New value of Visit matrix: 21
New value of Q matrix: 12.263
New value of Value function: 43.3685
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 416
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 285
New value of Q matrix: 43.4818
New value of Value function: 43.4818
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 417
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 286
New value of Q matrix: 43.5948
New value of Value function: 43.5948
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 418
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 287
New value of Q matrix: 43.7076
New value of Value function: 43.7076
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 419
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 288
New value of Q matrix: 43.8202
New value of Value function: 43.8202
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 420
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 289
New value of Q matrix: 43.9326
New value of Value function: 43.9326
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 421
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 290
New value of Q matrix: 44.0447
New value of Value function: 44.0447
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 422
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 291
New value of Q matrix: 44.1566
New value of Value function: 44.1566
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 423
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 292
New value of Q matrix: 44.2683
New value of Value function: 44.2683
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 424
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 10
New value of Visit matrix: 32
New value of Q matrix: 15.4287
New value of Value function: 44.2683
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 425
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 293
New value of Q matrix: 44.3798
New value of Value function: 44.3798
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 426
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 294
New value of Q matrix: 44.491
New value of Value function: 44.491
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 427
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 22
New value of Q matrix: 11.1382
New value of Value function: 44.491
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 428
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 10
New value of Visit matrix: 23
New value of Q matrix: 11.9162
New value of Value function: 44.491
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 429
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 8
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 430
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 431
----------
State: 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 592
	Distance: 3
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 432
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 9
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 433
----------
State: 736
	Distance: 4
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 736
	Distance: 4
	Angle: -1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 434
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 435
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 10
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 436
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 11
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 437
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 12
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 438
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 13
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 439
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 14
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 440
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 15
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 441
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 16
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 442
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 17
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 443
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 18
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 444
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 19
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 445
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 20
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 446
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 447
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 448
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 21
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 449
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 22
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 450
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 23
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 451
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 452
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 24
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 453
----------
State: 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 454
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 25
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 455
----------
State: 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 456
----------
State: 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 457
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 26
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 458
----------
State: 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 459
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 460
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 461
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 462
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 5
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 463
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 27
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 464
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 465
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 28
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 466
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 29
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 467
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 30
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 468
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 5
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 469
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 5
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 470
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 6
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 471
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 31
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 472
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 32
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 473
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 33
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 474
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 34
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 475
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 6
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 476
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 35
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 477
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 36
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 478
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 37
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 479
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 38
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 480
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 39
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 481
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 40
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 482
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 41
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 483
----------
State: 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 484
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 485
----------
State: 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 486
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Visit matrix: 7
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 487
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 42
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 488
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 6
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 489
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 43
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 490
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 44
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 491
----------
State: 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 492
----------
State: 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 493
----------
State: 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 494
----------
State: 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 495
----------
State: 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 496
----------
State: 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 444
	Distance: 2
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 497
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 45
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 498
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 46
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 499
----------
State: 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 156
	Distance: 0
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 500
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 47
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 501
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 48
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 502
----------
State: 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 3
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 503
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 295
New value of Q matrix: 44.602
New value of Value function: 44.602
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 504
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 296
New value of Q matrix: 44.7128
New value of Value function: 44.7128
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 505
----------
State: 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 506
----------
State: 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 588
	Distance: 3
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 49
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 507
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 297
New value of Q matrix: 44.8234
New value of Value function: 44.8234
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 508
----------
State: 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 4
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 509
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 298
New value of Q matrix: 44.9338
New value of Value function: 44.9338
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 510
----------
State: 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 2
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 511
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 299
New value of Q matrix: 45.0439
New value of Value function: 45.0439
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 512
----------
State: 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 732
	Distance: 4
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Visit matrix: 1
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 513
----------
State: 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 5
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 514
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 300
New value of Q matrix: 45.1538
New value of Value function: 45.1538
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 515
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 301
New value of Q matrix: 45.2635
New value of Value function: 45.2635
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 516
----------
State: 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 6
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 517
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 302
New value of Q matrix: 45.373
New value of Value function: 45.373
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 518
----------
State: 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
State': 876
	Distance: 5
	Angle: -1
	Height: 2
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 0
New value of Visit matrix: 7
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 519
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 10
New value of Visit matrix: 303
New value of Q matrix: 45.4822
New value of Value function: 45.4822
New value of Policy matrix: 1

