=======================================
Simulation: 1
Iteration: 1
----------
State: 1
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 1
State': 1
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 1
Action: 0
	Move front
Reward: -15
New value of Q matrix: -0.3
New value of Value function: 0
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 2
----------
State: 1
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 1
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -0.3
New value of Value function: 0
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 3
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 4
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -15
New value of Q matrix: -0.3
New value of Value function: 0
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 5
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 6
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -0.3
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 7
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -0.3
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 8
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 9
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -0.594
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 10
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 11
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 12
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 13
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 14
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 15
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -0.594
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 16
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -0.88212
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 17
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -1.16448
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 18
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 19
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 20
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 21
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 22
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -0.88212
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 23
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 24
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 25
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -1.44119
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 26
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 27
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 28
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -1.71236
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 29
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 30
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 31
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -1.16448
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 32
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 33
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -1.97812
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 34
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 35
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 36
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 37
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 38
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 39
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 40
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -2.23855
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 41
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 42
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 43
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 44
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 45
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 46
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 47
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 48
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 49
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -1.44119
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 50
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -1.71236
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 51
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 52
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 53
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 54
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 55
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 56
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 57
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 58
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 59
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 60
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 61
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 62
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 63
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 64
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -2.49378
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 65
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 66
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 67
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 68
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 69
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 70
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 71
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -1.97812
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 72
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -2.23855
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 73
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -2.49378
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 74
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 75
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 76
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 77
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -2.74391
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 78
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 79
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -2.98903
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 80
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 81
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -2.74391
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 82
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 83
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 84
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 85
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 86
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 87
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 88
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 89
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 90
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 91
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 92
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -3.22925
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 93
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 94
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 95
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 96
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 97
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 98
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 99
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 100
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 101
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 102
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -2.98903
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 103
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 104
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 105
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 106
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 107
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -3.22925
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 108
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -3.46466
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 109
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 110
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 111
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 112
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 113
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 114
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 115
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 116
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 117
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 118
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 119
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -15
New value of Q matrix: -3.69537
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 120
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 121
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 122
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 123
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 124
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -15
New value of Q matrix: -3.46466
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 125
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 126
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 127
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 128
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 129
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 130
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 131
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 0.34
New value of Value function: 0.34
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 132
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 0.67932
New value of Value function: 0.67932
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 133
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 1.01796
New value of Value function: 1.01796
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 134
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 1.35593
New value of Value function: 1.35593
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 135
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 1.69321
New value of Value function: 1.69321
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 136
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 2.02983
New value of Value function: 2.02983
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 137
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 2.36577
New value of Value function: 2.36577
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 138
----------
State: 784
	Distance: 4
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 784
	Distance: 4
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 14.5
New value of Q matrix: 0.29
New value of Value function: 0.29
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 139
----------
State: 64
	Distance: -1
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 64
	Distance: -1
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -15
New value of Q matrix: -0.3
New value of Value function: 0
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 140
----------
State: 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
State': 0
	Distance: -1
	Angle: -1
	Height: -1
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 0
New value of Q matrix: 0
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 141
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 2.70104
New value of Value function: 2.70104
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 142
----------
State: 496
	Distance: 2
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 496
	Distance: 2
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 19.5
New value of Q matrix: 0.39
New value of Value function: 0.39
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 143
----------
State: 496
	Distance: 2
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 496
	Distance: 2
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 19.5
New value of Q matrix: 0.39702
New value of Value function: 0.39702
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 144
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 3.03563
New value of Value function: 3.03563
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 145
----------
State: 496
	Distance: 2
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 496
	Distance: 2
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 19.5
New value of Q matrix: 0.397146
New value of Value function: 0.397146
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 146
----------
State: 496
	Distance: 2
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 496
	Distance: 2
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 19.5
New value of Q matrix: 0.786352
New value of Value function: 0.786352
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 147
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 17
New value of Q matrix: 0.394641
New value of Value function: 3.03563
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 148
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 3.36956
New value of Value function: 3.36956
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 149
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 17
New value of Q matrix: 0.787401
New value of Value function: 3.36956
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 150
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 3.70282
New value of Value function: 3.70282
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 151
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 4.03542
New value of Value function: 4.03542
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 152
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 4.36735
New value of Value function: 4.36735
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 153
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 17
New value of Q matrix: 1.19026
New value of Value function: 4.36735
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 154
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 0.14
New value of Value function: 0.14
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 155
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 0.27972
New value of Value function: 0.27972
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 156
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 17
New value of Q matrix: 0.418612
New value of Value function: 4.36735
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 157
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 17
New value of Q matrix: 0.418612
New value of Value function: 4.36735
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 158
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 0.419161
New value of Value function: 0.419161
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 159
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 17
New value of Q matrix: 0.828852
New value of Value function: 4.36735
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 160
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 17
New value of Q matrix: 1.58507
New value of Value function: 4.36735
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 161
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 0.558322
New value of Value function: 0.558322
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 162
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 0.697206
New value of Value function: 0.697206
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 163
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 0.835811
New value of Value function: 0.835811
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 164
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 0.97414
New value of Value function: 0.97414
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 165
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: 7
New value of Q matrix: 0.157535
New value of Value function: 0.97414
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 166
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 17
New value of Q matrix: 4.69861
New value of Value function: 4.69861
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 167
----------
State: 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 640
	Distance: 3
	Angle: 1
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 17
New value of Q matrix: 1.97795
New value of Value function: 4.69861
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 168
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 7
New value of Q matrix: 0.157535
New value of Value function: 0.97414
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 169
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 1.11219
New value of Value function: 1.11219
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 170
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 1.24997
New value of Value function: 1.24997
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 171
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 1.38747
New value of Value function: 1.38747
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 172
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 1.52469
New value of Value function: 1.52469
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 173
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 1.66164
New value of Value function: 1.66164
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 174
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 1.79832
New value of Value function: 1.79832
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 175
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 9.5
New value of Q matrix: 0.19
New value of Value function: 0.19
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 176
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 9.5
New value of Q matrix: 0.19342
New value of Value function: 0.19342
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 177
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 1.93472
New value of Value function: 1.93472
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 178
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 0.193482
New value of Value function: 0.193482
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 179
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 7
New value of Q matrix: 0.174825
New value of Value function: 1.93472
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 180
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 0.383095
New value of Value function: 0.383095
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 181
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 0.572328
New value of Value function: 0.572328
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 182
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 9.5
New value of Q matrix: 0.389854
New value of Value function: 0.572328
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 183
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 7
New value of Q matrix: 0.346154
New value of Value function: 1.93472
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 184
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 2.07085
New value of Value function: 2.07085
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 185
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 0.761184
New value of Value function: 0.761184
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 186
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 7
New value of Q matrix: 0.331659
New value of Value function: 2.07085
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 187
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 7
New value of Q matrix: 0.177275
New value of Value function: 2.07085
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 188
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 2.20671
New value of Value function: 2.20671
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 189
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 0.949661
New value of Value function: 0.949661
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 190
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 2.3423
New value of Value function: 2.3423
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 191
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 7
New value of Q matrix: 0.521392
New value of Value function: 2.3423
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 192
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 2.47761
New value of Value function: 2.47761
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 193
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 7
New value of Q matrix: 0.509623
New value of Value function: 2.47761
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 194
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 2.61266
New value of Value function: 2.61266
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 195
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 2.74743
New value of Value function: 2.74743
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 196
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 2.88194
New value of Value function: 2.88194
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 197
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 3.01617
New value of Value function: 3.01617
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 198
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 1.13776
New value of Value function: 1.13776
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 199
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 1.32549
New value of Value function: 1.32549
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 200
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 9.5
New value of Q matrix: 0.595915
New value of Value function: 1.32549
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 201
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 1.51284
New value of Value function: 1.51284
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 202
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 1.69981
New value of Value function: 1.69981
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 203
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 9.5
New value of Q matrix: 0.220597
New value of Value function: 1.69981
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 204
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 3.15014
New value of Value function: 3.15014
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 205
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 1.88641
New value of Value function: 1.88641
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 206
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 9.5
New value of Q matrix: 0.807952
New value of Value function: 1.88641
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 207
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 2.07264
New value of Value function: 2.07264
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 208
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -0.5
New value of Q matrix: -0.01
New value of Value function: 0
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 209
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 3.28384
New value of Value function: 3.28384
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 210
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 3.41727
New value of Value function: 3.41727
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 211
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 2.25849
New value of Value function: 2.25849
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 212
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 2.44398
New value of Value function: 2.44398
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 213
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 2.62909
New value of Value function: 2.62909
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 214
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 2.81383
New value of Value function: 2.81383
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 215
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 9.5
New value of Q matrix: 0.456834
New value of Value function: 2.81383
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 216
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -0.5
New value of Q matrix: -0.01
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 217
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -0.5
New value of Q matrix: -0.01
New value of Value function: 0
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 218
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.06
New value of Value function: 0
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 219
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.06
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 220
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -0.5
New value of Q matrix: -0.0198
New value of Value function: 0
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 221
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -0.5
New value of Q matrix: -0.029404
New value of Value function: 0
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 222
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.06
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 223
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.06
New value of Value function: 0
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 224
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.06
New value of Value function: -0.06
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 225
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.11988
New value of Value function: -0.06
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 226
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -0.5
New value of Q matrix: -0.0198
New value of Value function: 0
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 227
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.178562
New value of Value function: -0.06
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 228
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -0.5
New value of Q matrix: -0.01
New value of Value function: 0
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 229
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.11988
New value of Value function: -0.06
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 230
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.236071
New value of Value function: -0.06
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 231
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.11988
New value of Value function: -0.06
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 232
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.11988
New value of Value function: -0.06
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 233
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.11988
New value of Value function: -0.11988
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 234
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.17964
New value of Value function: -0.11988
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 235
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.17964
New value of Value function: -0.11988
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 236
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.17964
New value of Value function: -0.11988
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 237
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.238205
New value of Value function: -0.11988
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 238
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.295599
New value of Value function: -0.11988
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 239
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -0.5
New value of Q matrix: -0.01
New value of Value function: -0.01
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 240
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.17964
New value of Value function: -0.17964
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 241
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.239281
New value of Value function: -0.17964
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 242
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -0.5
New value of Q matrix: -0.01998
New value of Value function: -0.01
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 243
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.239281
New value of Value function: -0.17964
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 244
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -0.5
New value of Q matrix: -0.01998
New value of Value function: -0.01
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 245
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -0.5
New value of Q matrix: -0.01998
New value of Value function: -0.0198
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 246
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -0.5
New value of Q matrix: -0.0297604
New value of Value function: -0.01998
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 247
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.352921
New value of Value function: -0.17964
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 248
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.409096
New value of Value function: -0.17964
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 249
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.464147
New value of Value function: -0.17964
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 250
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -13
New value of Q matrix: -0.26
New value of Value function: 0
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 251
----------
State: 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -10.5
New value of Q matrix: -0.21
New value of Value function: 0
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 252
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -13
New value of Q matrix: -0.26
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 253
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -13
New value of Q matrix: -0.26
New value of Value function: 0
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 254
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -13
New value of Q matrix: -0.26
New value of Value function: 0
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 255
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -13
New value of Q matrix: -0.5148
New value of Value function: 0
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 256
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -13
New value of Q matrix: -0.764504
New value of Value function: 0
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 257
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -13
New value of Q matrix: -0.26
New value of Value function: -0.26
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 258
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -13
New value of Q matrix: -0.51948
New value of Value function: -0.26
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 259
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -13
New value of Q matrix: -0.51948
New value of Value function: -0.26
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 260
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -13
New value of Q matrix: -0.51948
New value of Value function: -0.26
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 261
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -13
New value of Q matrix: -0.51948
New value of Value function: -0.51948
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 262
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -13
New value of Q matrix: -0.778441
New value of Value function: -0.51948
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 263
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -13
New value of Q matrix: -0.778441
New value of Value function: -0.51948
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 264
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -13
New value of Q matrix: -0.778441
New value of Value function: -0.51948
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 265
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -13
New value of Q matrix: -1.03222
New value of Value function: -0.51948
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 266
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.239281
New value of Value function: -0.236071
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 267
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.295599
New value of Value function: -0.239281
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 268
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.298802
New value of Value function: -0.239281
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 269
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -0.5
New value of Q matrix: -0.02994
New value of Value function: -0.01998
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 270
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.519171
New value of Value function: -0.239281
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 271
----------
State: 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -10.5
New value of Q matrix: -0.21
New value of Value function: 0
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 272
----------
State: 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -10.5
New value of Q matrix: -0.21
New value of Value function: 0
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 273
----------
State: 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -10.5
New value of Q matrix: -0.21
New value of Value function: 0
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 274
----------
State: 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -10.5
New value of Q matrix: -0.21
New value of Value function: -0.21
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 275
----------
State: 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -10.5
New value of Q matrix: -0.41958
New value of Value function: -0.21
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 276
----------
State: 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -10.5
New value of Q matrix: -0.41958
New value of Value function: -0.21
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 277
----------
State: 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -10.5
New value of Q matrix: -0.41958
New value of Value function: -0.21
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 278
----------
State: 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 712
	Distance: 3
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -13
New value of Q matrix: -0.778441
New value of Value function: -0.764504
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 279
----------
State: 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -10.5
New value of Q matrix: -0.41958
New value of Value function: -0.21
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 280
----------
State: 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 568
	Distance: 2
	Angle: 4
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -10.5
New value of Q matrix: -0.624968
New value of Value function: -0.21
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 281
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -0.5
New value of Q matrix: -0.02994
New value of Value function: -0.01998
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 282
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -0.5
New value of Q matrix: -0.02994
New value of Value function: -0.029404
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 283
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.298802
New value of Value function: -0.239281
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 284
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.298802
New value of Value function: -0.295599
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 285
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.355008
New value of Value function: -0.298802
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 286
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.358205
New value of Value function: -0.298802
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 287
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.358205
New value of Value function: -0.298802
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 288
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.358205
New value of Value function: -0.355008
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 289
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.414298
New value of Value function: -0.358205
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 290
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.417488
New value of Value function: -0.358205
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 291
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -0.5
New value of Q matrix: -0.0393452
New value of Value function: -0.0297604
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 292
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.417488
New value of Value function: -0.358205
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 293
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -0.5
New value of Q matrix: -0.0397009
New value of Value function: -0.02994
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 294
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.417488
New value of Value function: -0.414298
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 295
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.473469
New value of Value function: -0.417488
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 296
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.476653
New value of Value function: -0.417488
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 297
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.476653
New value of Value function: -0.417488
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 298
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 7
New value of Q matrix: 0.700942
New value of Value function: 3.41727
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 299
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.476653
New value of Value function: -0.473469
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 300
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -0.5
New value of Q matrix: -0.0490972
New value of Value function: -0.02994
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 301
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.532522
New value of Value function: -0.476653
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 302
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.5357
New value of Value function: -0.476653
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 303
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.5357
New value of Value function: -0.476653
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 304
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.5357
New value of Value function: -0.519171
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 305
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.578133
New value of Value function: -0.532522
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 306
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.591457
New value of Value function: -0.5357
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 307
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -0.5
New value of Q matrix: -0.0398802
New value of Value function: -0.02994
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 308
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.594629
New value of Value function: -0.5357
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 309
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.594629
New value of Value function: -0.5357
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 310
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 3.55044
New value of Value function: 3.55044
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 311
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 2.9982
New value of Value function: 2.9982
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 312
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 3.18221
New value of Value function: 3.18221
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 313
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 9.5
New value of Q matrix: 1.03907
New value of Value function: 3.18221
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 314
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 3.68334
New value of Value function: 3.68334
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 315
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 7
New value of Q matrix: 0.717264
New value of Value function: 3.68334
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 316
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 3.36584
New value of Value function: 3.36584
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 317
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 3.54911
New value of Value function: 3.54911
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 318
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 9.5
New value of Q matrix: 1.27218
New value of Value function: 3.54911
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 319
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 3.81597
New value of Value function: 3.81597
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 320
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: 7
New value of Q matrix: 0.89561
New value of Value function: 3.81597
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 321
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -0.5
New value of Q matrix: -0.0398802
New value of Value function: -0.02994
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 322
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.594629
New value of Value function: -0.578133
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 323
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.650034
New value of Value function: -0.578133
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 324
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -0.5
New value of Q matrix: -0.0494458
New value of Value function: -0.02994
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 325
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -0.5
New value of Q matrix: -0.0398802
New value of Value function: -0.0398802
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 326
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: -3
New value of Q matrix: -0.653143
New value of Value function: -0.578133
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 327
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.636977
New value of Value function: -0.594629
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 328
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: -3
New value of Q matrix: -0.653439
New value of Value function: -0.594629
New value of Policy matrix: 3

=======================================
Simulation: 1
Iteration: 329
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.653439
New value of Value function: -0.636977
New value of Policy matrix: 2

=======================================
Simulation: 1
Iteration: 330
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 2
	Turn left
Reward: -3
New value of Q matrix: -0.695703
New value of Value function: -0.650034
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 331
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -3
New value of Q matrix: -0.712071
New value of Value function: -0.650034
New value of Policy matrix: 1

=======================================
Simulation: 1
Iteration: 332
----------
State: 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 544
	Distance: 2
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 3
	Turn right
Reward: -0.5
New value of Q matrix: -0.0498004
New value of Value function: -0.0398802
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 333
----------
State: 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 688
	Distance: 3
	Angle: 3
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: -3
New value of Q matrix: -0.708734
New value of Value function: -0.653143
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 334
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 3.94834
New value of Value function: 3.94834
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 335
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 4.08044
New value of Value function: 4.08044
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 336
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 1
	Move back
Reward: 7
New value of Q matrix: 0.387178
New value of Value function: 4.08044
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 337
----------
State: 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 664
	Distance: 3
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 0
	Move front
Reward: 7
New value of Q matrix: 4.21228
New value of Value function: 4.21228
New value of Policy matrix: 0

=======================================
Simulation: 1
Iteration: 338
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 3.73201
New value of Value function: 3.73201
New value of Policy matrix: 4

=======================================
Simulation: 1
Iteration: 339
----------
State: 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
State': 520
	Distance: 2
	Angle: 2
	Height: 3
	Object picked: 0
	Arm folded: 0
Action: 4
	Move arm
Reward: 9.5
New value of Q matrix: 3.91455
New value of Value function: 3.91455
New value of Policy matrix: 4

